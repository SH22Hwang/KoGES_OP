# 실험 결과 정리

## 통계 수치 정리

폐경 후 XGBoost

|                          | Accuracy | Precision | Recall | F1-score |
| ------------------------ | -------- | --------- | ------ | -------- |
| x_train, test, val 각각 증강 | 0.747    | 0.759     | 0.724  | 0.741    |
| 전체 증강                    | 0.775    | 0.785     | 0.759  | 0.772    |
| 상위 20개 모델                | 0.782    | 0.780     | 0.786  | 0.783    |

폐경 전 XGBoost

|                          | Accuracy | Precision | Recall | F1-score |
| ------------------------ | -------- | --------- | ------ | -------- |
| x_train, test, val 각각 증강 | 0.776    | 0.779     | 0.771  | 0.775    |
| 전체 증강                    | 0.898    | 0.856     | 0.956  | 0.903    |
| 상위 20개 모델                | 0.920    | 0.897     | 0.949  | 0.922    |

## 실험 결과 해설

- x_train, test, val 각각 증강
  
  1. train, test, val 분리.
  2. train, test 정규화
  3. SMOTE를 이용하여 train / test 따로 증강

- 전체 증강
  
  1. SMOTE를 이용하여 전체(x, y일 때) 증강
  2. 증강한 데이터를 train, test, val 분리

- 상위 20개 모델
  
  - Feature Importance 상위 20개 추출
    
    - 전체 증강 케이스의 XGBoost의 Featuer importance 활용.
    - 각각 증강한 모델은 Featuer importance가 일관적이지 않았음.
    - 전체 증강한 모델의 상위 20개가 각각 증강한 모델의 상위20개보다 더 성능 향상에 도움이 됨
  
  - 상위 20개 추출후 새로운 XGBoost 모델 학습

## 상위 20개 변수 목록

- 민경 누나가 보여줄 예정

## 향후 계획

- XGB외 다른 모델 실험 및 표로 작성
  
  - 의사결정나무
  
  - random forest
  
  - SVM
  
  - LGBM
  
  - MLP

- method 작성
