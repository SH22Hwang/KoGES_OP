# 변수 자르기 대작전

## 방법

1. 전체 x 증강 후 xgboost 학습

2. xgboost의 feature importance 추출

3. 상위 20개만을 이용하여 다시 학습

4. 1번과 3번의 f1-score 비교

## 실험

- x_train만 증강 후 xgboost : 자료 삭제

> 최소: 0.3
> 최대: 0.34782608695652173
> 평균: 0.3262824444034863

- 전체 증강 후 xgboost

> Confusion Matrix:
> [[292 77]
>  [ 89 281]]
>  [[TP FN]
>  [ FP TN]]
>  Accuracy: 0.775
>  Precison: 0.785
>  Recall : 0.759
>  F1 Score: 0.772

- feature importance
  
  ![fig1.png](C:\GIthub\KoGES_OP\Docs\231120\fig1.png)

- 상위 20개 목록

> S1_HYST 0.009873
> AS1_GT 0.009998
> AS1_TOTPRT 0.010084
> AS1_WEIGHT 0.010143
> AS1_ENERGY 0.010235
> AS1_BPSIT1RD 0.010397
> AS1_CARBO 0.010594
> AS1_LP 0.010786
> AS1_AGE 0.010885
> AS1_PREG 0.010990
> AS1_CA_U 0.011435
> AS1_BPSIT1LS 0.012847
> AS1_CHILD_P 0.013625
> SNP_A-1809518 0.013976
> AS1_PMAG_C 0.014113
> SNP_A-1922415 0.014502
> AS1_BDFTR 0.019050
> AS1_HT 0.022881
> AS1_FMOSREL_S 0.023556
> AS1_ARRM 0.038178
